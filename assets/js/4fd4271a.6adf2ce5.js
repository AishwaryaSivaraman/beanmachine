"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3744],{3905:function(e,n,a){a.r(n),a.d(n,{MDXContext:function(){return c},MDXProvider:function(){return p},mdx:function(){return h},useMDXComponents:function(){return m},withMDXComponents:function(){return d}});var t=a(67294);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function r(){return r=Object.assign||function(e){for(var n=1;n<arguments.length;n++){var a=arguments[n];for(var t in a)Object.prototype.hasOwnProperty.call(a,t)&&(e[t]=a[t])}return e},r.apply(this,arguments)}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function s(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){i(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function l(e,n){if(null==e)return{};var a,t,i=function(e,n){if(null==e)return{};var a,t,i={},r=Object.keys(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var c=t.createContext({}),d=function(e){return function(n){var a=m(n.components);return t.createElement(e,r({},n,{components:a}))}},m=function(e){var n=t.useContext(c),a=n;return e&&(a="function"==typeof e?e(n):s(s({},n),e)),a},p=function(e){var n=m(e.components);return t.createElement(c.Provider,{value:n},e.children)},u={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},f=t.forwardRef((function(e,n){var a=e.components,i=e.mdxType,r=e.originalType,o=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=m(a),p=i,f=d["".concat(o,".").concat(p)]||d[p]||u[p]||r;return a?t.createElement(f,s(s({ref:n},c),{},{components:a})):t.createElement(f,s({ref:n},c))}));function h(e,n){var a=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=f;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=a[c];return t.createElement.apply(null,o)}return t.createElement.apply(null,a)}f.displayName="MDXCreateElement"},28938:function(e,n,a){a.r(n),a.d(n,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return d},default:function(){return p}});var t=a(87462),i=a(63366),r=(a(67294),a(3905)),o=["components"],s={id:"bean_machine_advantages",title:"Bean Machine Advantages",sidebar_label:"Bean Machine Advantages"},l=void 0,c={unversionedId:"overview/bean_machine_advantages/bean_machine_advantages",id:"overview/bean_machine_advantages/bean_machine_advantages",isDocsHomePage:!1,title:"Bean Machine Advantages",description:"If you're new to probabilistic programming languages, we recommend you skip to the next page!",source:"@site/../docs/overview/bean_machine_advantages/bean_machine_advantages.md",sourceDirName:"overview/bean_machine_advantages",slug:"/overview/bean_machine_advantages/bean_machine_advantages",permalink:"/docs/overview/bean_machine_advantages/bean_machine_advantages",editUrl:"https://github.com/facebook/docusaurus/edit/master/website/../docs/overview/bean_machine_advantages/bean_machine_advantages.md",tags:[],version:"current",frontMatter:{id:"bean_machine_advantages",title:"Bean Machine Advantages",sidebar_label:"Bean Machine Advantages"},sidebar:"someSidebar",previous:{title:"Introduction",permalink:"/docs/introduction"},next:{title:"Quick Start",permalink:"/docs/quickstart"}},d=[{value:"Site-based inference",id:"site-based-inference",children:[],level:2},{value:"Declarative modeling",id:"declarative-modeling",children:[],level:2},{value:"Programmable inference",id:"programmable-inference",children:[],level:2},{value:"Advanced methods",id:"advanced-methods",children:[],level:2},{value:"Bean Machine Graph compilation",id:"bean-machine-graph-compilation",children:[],level:2}],m={toc:d};function p(e){var n=e.components,a=(0,i.Z)(e,o);return(0,r.mdx)("wrapper",(0,t.Z)({},m,a,{components:n,mdxType:"MDXLayout"}),(0,r.mdx)("div",{className:"admonition admonition-tip alert alert--success"},(0,r.mdx)("div",{parentName:"div",className:"admonition-heading"},(0,r.mdx)("h5",{parentName:"div"},(0,r.mdx)("span",{parentName:"h5",className:"admonition-icon"},(0,r.mdx)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,r.mdx)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"tip")),(0,r.mdx)("div",{parentName:"div",className:"admonition-content"},(0,r.mdx)("p",{parentName:"div"},"If you're new to probabilistic programming languages, we recommend you skip to the ",(0,r.mdx)("a",{parentName:"p",href:"/docs/quickstart"},"next page"),"!"))),(0,r.mdx)("p",null,"By building on top of PyTorch with a declarative modeling syntax, Bean Machine can be simultaneously performant and intuitive for building probabilistic models. Bean Machine provides further value by implementing cutting-edge inference algorithms and allowing the user to select and program custom inferences for different problems and subproblems."),(0,r.mdx)("h2",{id:"site-based-inference"},"Site-based inference"),(0,r.mdx)("p",null,'Bean Machine uses a site-based inference engine. "Sites" are random variable families, and Bean Machine uses these families to enable a modular inference engine.'),(0,r.mdx)("p",null,'The simplest form of site-based inference is called "single-site" inference. In the single-site paradigm, models are built up from random variables that can be reasoned about individually. Bean Machine can exploit this modularity to update random variables one-at-a-time, reducing unnecessary computation and enabling posterior updates that might not be possible if processing the entire model in one go.'),(0,r.mdx)("p",null,'Bean Machine also supports "multi-site" inference, in which multiple sites are reasoned about jointly. This increases complexity during inference, but it allows the engine to exploit inter-site correlations when fitting the posterior distribution.'),(0,r.mdx)("p",null,"Altogether, site-based inference is a flexible pattern for trading off complexity and modularity, and enables the advanced techniques outlined below."),(0,r.mdx)("h2",{id:"declarative-modeling"},"Declarative modeling"),(0,r.mdx)("p",null,"In Bean Machine, random variables are implemented as decorated Python functions, which naturally form an interface for the model. Using functions makes it simple to determine a random variable's definition, since it is contained in a function that is usually only a few lines long. This lets you directly refer to random variables to access inferred distributions or when binding data to your model. This is safer and more natural than relying on string identifiers, and also enables IDE support and type-checking in many cases."),(0,r.mdx)("p",null,"Declarative modeling also frees the inference engine to reorder model execution. Foremost, it enables computation of immediate dependencies for random variables. This makes it possible to propose new values for a random variable by examining only its dependencies, saving significant amounts of compute in models with complex structure."),(0,r.mdx)("h2",{id:"programmable-inference"},"Programmable inference"),(0,r.mdx)("p",null,"Bean Machine allows the user to design and apply powerful inference methods. Because Bean Machine can propose updates for random variables individually, the user is free to customize the ",(0,r.mdx)("em",{parentName:"p"},"method"),' which it uses to propose those values. Different inference methods can be supplied for different families of random variables. For example, a particular model can leverage gradient information when proposing values for differentiable random variables, and at the same time might sample from discrete ones with a particle filter. This single-site "compositional inference" pattern enables seamless interoperation among any MCMC-based inference strategies.'),(0,r.mdx)("p",null,'Though powerful, compositional inference limits Bean Machine\'s global understanding of the model. To combat this, Bean Machine exposes a separate functionality to allow joint processing of multiple sites. This "multi-site inference" causes Bean Machine to process both sites together before updating either, which is especially useful for updating highly-correlated random variables. Certain inference methods may be able to further exploit multi-sites with inference-specific optimizations.  Since multi-site inference is orthogonal to compositional inference, it allows you to create sophisticated, model-specific inference strategies with virtually no additional effort.'),(0,r.mdx)("h2",{id:"advanced-methods"},"Advanced methods"),(0,r.mdx)("p",null,"Bean Machine supports a variety of classic inference methods such as ancestral sampling and the No-U-Turn sampler (NUTS). However, the framework also leverages single-site understanding of the model in order to provide efficient methods that take advantage of higher-order gradients and model structure."),(0,r.mdx)("p",null,"Bean Machine includes the first implementation of Newtonian Monte Carlo (NMC) in a more general platform. NMC utilizes second-order gradient information to construct a multivariate Gaussian proposer that takes local curvature into account. As such, it can produce sample very efficiently with no warmup period when the posterior is roughly Gaussian. Bean Machine's structural understanding of the model lets us keep computation relatively cheap by only modeling a subset of the space that is relevant to updating a particular random variable."),(0,r.mdx)("p",null,"For certain domains, prepackaged inference methods may not be the best tool for the job. For example, if dealing with a problem specified in spherical coordinates, it may be useful to incorporate a notion of spherical locality into the inference proposal. Or, you may want to incorporate some notion of ordering when dealing with certain discrete random variables. Bean Machine exposes a flexible abstraction called ",(0,r.mdx)("em",{parentName:"p"},"custom proposers")," for just this problem. Custom proposers let the user design powerful new inference methods from the building blocks of existing ones, while easily plugging into Bean Machine's multi-site paradigm."),(0,r.mdx)("h2",{id:"bean-machine-graph-compilation"},"Bean Machine Graph compilation"),(0,r.mdx)("p",null,"PyTorch offers strong performance for models comprised of a small number of large tensors. However, many probabilistic models have a rich or sparse structure that is difficult to write in terms of just a handful of large tensor operations. And in many cases, these are exactly the problems for which probabilistic modeling is most compelling!"),(0,r.mdx)("p",null,"To address this, we are developing an experimental inference runtime called Bean Machine Graph (BMG) Inference. BMG Inference is a specialized combination of compiler and a fast, independent runtime that is optimized to run inference even for un-tensorized models. By design, BMG Inference has the same interface as other Bean Machine inference methods, relying on a custom behind-the-scenes compiler to interpret your model and translate it to a faster implementation with no Python dependencies."),(0,r.mdx)("p",null,"BMG Inference routinely achieves 1 to 2 orders-of-magnitude speedup for untensorized models. However, please note that this infrastructure is under development, and the supported feature set may be limited."))}p.isMDXComponent=!0}}]);