"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[9398],{3905:function(e,a,n){n.r(a),n.d(a,{MDXContext:function(){return l},MDXProvider:function(){return c},mdx:function(){return h},useMDXComponents:function(){return d},withMDXComponents:function(){return o}});var t=n(67294);function s(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function m(){return m=Object.assign||function(e){for(var a=1;a<arguments.length;a++){var n=arguments[a];for(var t in n)Object.prototype.hasOwnProperty.call(n,t)&&(e[t]=n[t])}return e},m.apply(this,arguments)}function r(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function p(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?r(Object(n),!0).forEach((function(a){s(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function i(e,a){if(null==e)return{};var n,t,s=function(e,a){if(null==e)return{};var n,t,s={},m=Object.keys(e);for(t=0;t<m.length;t++)n=m[t],a.indexOf(n)>=0||(s[n]=e[n]);return s}(e,a);if(Object.getOwnPropertySymbols){var m=Object.getOwnPropertySymbols(e);for(t=0;t<m.length;t++)n=m[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var l=t.createContext({}),o=function(e){return function(a){var n=d(a.components);return t.createElement(e,m({},a,{components:n}))}},d=function(e){var a=t.useContext(l),n=a;return e&&(n="function"==typeof e?e(a):p(p({},a),e)),n},c=function(e){var a=d(e.components);return t.createElement(l.Provider,{value:a},e.children)},N={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},x=t.forwardRef((function(e,a){var n=e.components,s=e.mdxType,m=e.originalType,r=e.parentName,l=i(e,["components","mdxType","originalType","parentName"]),o=d(n),c=s,x=o["".concat(r,".").concat(c)]||o[c]||N[c]||m;return n?t.createElement(x,p(p({ref:a},l),{},{components:n})):t.createElement(x,p({ref:a},l))}));function h(e,a){var n=arguments,s=a&&a.mdxType;if("string"==typeof e||s){var m=n.length,r=new Array(m);r[0]=x;var p={};for(var i in a)hasOwnProperty.call(a,i)&&(p[i]=a[i]);p.originalType=e,p.mdxType="string"==typeof e?e:s,r[1]=p;for(var l=2;l<m;l++)r[l]=n[l];return t.createElement.apply(null,r)}return t.createElement.apply(null,n)}x.displayName="MDXCreateElement"},3650:function(e,a,n){n.r(a),n.d(a,{frontMatter:function(){return p},contentTitle:function(){return i},metadata:function(){return l},toc:function(){return o},default:function(){return c}});var t=n(87462),s=n(63366),m=(n(67294),n(3905)),r=["components"],p={id:"analysis",title:"Analysis",sidebar_label:"Analysis"},i=void 0,l={unversionedId:"overview/analysis/analysis",id:"overview/analysis/analysis",isDocsHomePage:!1,title:"Analysis",description:"Inference results are useful not only for learning from your posterior distributions, but for verifying that inference ran correctly. We'll cover common techniques for analyzing results in this section.",source:"@site/../docs/overview/analysis/analysis.md",sourceDirName:"overview/analysis",slug:"/overview/analysis/analysis",permalink:"/docs/overview/analysis/analysis",editUrl:"https://github.com/facebook/docusaurus/edit/master/website/../docs/overview/analysis/analysis.md",tags:[],version:"current",frontMatter:{id:"analysis",title:"Analysis",sidebar_label:"Analysis"},sidebar:"someSidebar",previous:{title:"Inference",permalink:"/docs/overview/inference/inference"},next:{title:"Jupyter Notebooks",permalink:"/docs/overview/tutorials/tutorials"}},o=[{value:"Results of inference",id:"results-of-inference",children:[{value:"Extracting samples for a specific variable",id:"extracting-samples-for-a-specific-variable",children:[],level:3},{value:"Extracting samples for a specific chain",id:"extracting-samples-for-a-specific-chain",children:[],level:3},{value:"Visualizing distributions",id:"visualizing-distributions",children:[],level:3}],level:2},{value:'<a name="diagnostics"></a>Diagnostics',id:"diagnostics",children:[{value:"Summary statistics",id:"summary-statistics",children:[],level:3},{value:"Diagnostic plots",id:"diagnostic-plots",children:[],level:3}],level:2}],d={toc:o};function c(e){var a=e.components,p=(0,s.Z)(e,r);return(0,m.mdx)("wrapper",(0,t.Z)({},d,p,{components:a,mdxType:"MDXLayout"}),(0,m.mdx)("p",null,"Inference results are useful not only for learning from your posterior distributions, but for verifying that inference ran correctly. We'll cover common techniques for analyzing results in this section."),(0,m.mdx)("h2",{id:"results-of-inference"},"Results of inference"),(0,m.mdx)("p",null,"Bean Machine stores the results of inference in an object of type ",(0,m.mdx)("inlineCode",{parentName:"p"},"MonteCarloSamples"),". This class uses\nan underlying data structure of a dictionary mapping a model's random variables to a PyTorch tensor of sampled values. The class can be accessed like a dictionary, and there are additional wrapper methods to make function calls more explicit."),(0,m.mdx)("p",null,"In the ",(0,m.mdx)("a",{parentName:"p",href:"/docs/overview/inference/inference"},"Inference")," section, we obtained results on the disease modeling example by running inference:"),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre",className:"language-py"},"samples = bm.CompositionalInference().infer(\n    queries=[ reproduction_rate() ],\n    observations=observations,\n    num_samples=10000,\n    num_chains=4,\n)\nsamples\n")),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre"},"<beanmachine.ppl.inference.monte_carlo_samples.MonteCarloSamples>\n")),(0,m.mdx)("h3",{id:"extracting-samples-for-a-specific-variable"},"Extracting samples for a specific variable"),(0,m.mdx)("p",null,"We ran inference to compute the posterior for ",(0,m.mdx)("inlineCode",{parentName:"p"},"reproduction_rate()"),", since that was listed in ",(0,m.mdx)("inlineCode",{parentName:"p"},"queries"),". We can see that the posterior for ",(0,m.mdx)("inlineCode",{parentName:"p"},"reproduction_rate()")," (and only for ",(0,m.mdx)("inlineCode",{parentName:"p"},"reproduction_rate()"),") is available in ",(0,m.mdx)("inlineCode",{parentName:"p"},"samples"),":"),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre",className:"language-py"},"samples.get_rv_names()\n")),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre"},"[RVIdentifier(function=<function reproduction_rate>, arguments=())]\n")),(0,m.mdx)("p",null,"To extract the inference results for ",(0,m.mdx)("inlineCode",{parentName:"p"},"reproduction_rate()"),", we can use ",(0,m.mdx)("inlineCode",{parentName:"p"},"get_variable()"),":"),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre",className:"language-py"},"samples.get_variable(reproduction_rate())\n")),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre"},"tensor([[1.0000, 0.4386, 0.2751,  ..., 0.2177, 0.2177, 0.2193],\n        [0.2183, 0.2183, 0.2184,  ..., 0.2177, 0.2177, 0.2177],\n        [0.2170, 0.2180, 0.2183,  ..., 0.2180, 0.2180, 0.2180],\n        [0.2180, 0.2180, 0.2172,  ..., 0.2180, 0.2180, 0.2176]])\n")),(0,m.mdx)("p",null,"The result has shape ",(0,m.mdx)("span",{parentName:"p",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("mn",{parentName:"mrow"},"4"),(0,m.mdx)("mo",{parentName:"mrow"},"\xd7"),(0,m.mdx)("mn",{parentName:"mrow"},"10000")),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"4 \\times 10000")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.72777em",verticalAlign:"-0.08333em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},"4"),(0,m.mdx)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}}),(0,m.mdx)("span",{parentName:"span",className:"mbin"},"\xd7"),(0,m.mdx)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222222222222222em"}})),(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.64444em",verticalAlign:"0em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},"1"),(0,m.mdx)("span",{parentName:"span",className:"mord"},"0"),(0,m.mdx)("span",{parentName:"span",className:"mord"},"0"),(0,m.mdx)("span",{parentName:"span",className:"mord"},"0"),(0,m.mdx)("span",{parentName:"span",className:"mord"},"0"))))),", representing the 10,000 samples that we drew in each of the four chains of inference from the posterior distribution."),(0,m.mdx)("p",null,(0,m.mdx)("inlineCode",{parentName:"p"},"MonteCarloSamples")," supports convenient dictionary indexing syntax to obtain the same information:"),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre",className:"language-py"},"samples[ reproduction_rate() ]\n")),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre"},"tensor([[1.0000, 0.4386, 0.2751,  ..., 0.2177, 0.2177, 0.2193],\n        [0.2183, 0.2183, 0.2184,  ..., 0.2177, 0.2177, 0.2177],\n        [0.2170, 0.2180, 0.2183,  ..., 0.2180, 0.2180, 0.2180],\n        [0.2180, 0.2180, 0.2172,  ..., 0.2180, 0.2180, 0.2176]])\n")),(0,m.mdx)("p",null,"Please note that many inference methods require a small number of samples before they start drawing samples that correctly resemble the posterior distribution. We recommend you discard at least a few hundred samples before using your inference results."),(0,m.mdx)("h3",{id:"extracting-samples-for-a-specific-chain"},"Extracting samples for a specific chain"),(0,m.mdx)("p",null,"We'll see how to make use of chains in ",(0,m.mdx)("a",{parentName:"p",href:"#diagnostics"},"Diagnostics"),"; for inspecting the samples themselves, it is often useful to examine each chain individually. The recommended way to access the results of a specific chain is with ",(0,m.mdx)("inlineCode",{parentName:"p"},"get_chain()"),":"),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre",className:"language-py"},"chain = samples.get_chain(chain=0)\nchain\n")),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre"},"<beanmachine.ppl.inference.monte_carlo_samples.MonteCarloSamples>\n")),(0,m.mdx)("p",null,"This returns a new ",(0,m.mdx)("inlineCode",{parentName:"p"},"MonteCarloSamples")," object which is limited to the specified chain. Tensors no longer have a dimension representing the chain:"),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre",className:"language-py"},"chain[ reproduction_rate() ]\n")),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre"},"tensor([1.0000, 0.4386, 0.2751,  ..., 0.2177, 0.2177, 0.2193])\n")),(0,m.mdx)("h3",{id:"visualizing-distributions"},"Visualizing distributions"),(0,m.mdx)("p",null,"Visualizing the results of inference can be a great help in understanding them. Since you now know how to access posterior samples, you're free to use whatever visualization tools you prefer."),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre",className:"language-py"},'reproduction_rate_samples = samples[ reproduction_rate() ][0][100:]\nplt.hist(reproduction_rate_samples, label="reproduction_rate_samples")\nplt.axvline(reproduction_rate_samples.mean(), label=f"Posterior mean = {reproduction_rate_samples.mean() :.2f}", color="K")\nplt.xlabel("reproduction_rate")\nplt.ylabel("Probability density")\nplt.legend();\n')),(0,m.mdx)("p",null,(0,m.mdx)("img",{src:n(38016).Z})),(0,m.mdx)("h2",{id:"diagnostics"},(0,m.mdx)("a",{name:"diagnostics"}),"Diagnostics"),(0,m.mdx)("p",null,"After running inference it is useful to run diagnostic tools to assess reliability of the inference run. Bean Machine provides two standard types of such diagnostic tools, discussed below."),(0,m.mdx)("h3",{id:"summary-statistics"},"Summary statistics"),(0,m.mdx)("p",null,"Bean Machine provides important summary statistics for individual, numerically-valued random variables. Let's take a look at the code to generate them, and then we'll break down the statistics themselves."),(0,m.mdx)("p",null,"Bean Machine's ",(0,m.mdx)("inlineCode",{parentName:"p"},"Diagnostics")," object makes it easy to generate a Pandas ",(0,m.mdx)("inlineCode",{parentName:"p"},"DataFrame")," presenting these statistics for all queried random variables:"),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre",className:"language-py"},"bm.Diagnostics(samples).summary()\n")),(0,m.mdx)("table",null,(0,m.mdx)("thead",{parentName:"table"},(0,m.mdx)("tr",{parentName:"thead"},(0,m.mdx)("th",{parentName:"tr",align:null}),(0,m.mdx)("th",{parentName:"tr",align:null},"avg"),(0,m.mdx)("th",{parentName:"tr",align:null},"std"),(0,m.mdx)("th",{parentName:"tr",align:null},"2.5%"),(0,m.mdx)("th",{parentName:"tr",align:null},"50%"),(0,m.mdx)("th",{parentName:"tr",align:null},"97.5%"),(0,m.mdx)("th",{parentName:"tr",align:null},"r_hat"),(0,m.mdx)("th",{parentName:"tr",align:null},"n_eff"))),(0,m.mdx)("tbody",{parentName:"table"},(0,m.mdx)("tr",{parentName:"tbody"},(0,m.mdx)("td",{parentName:"tr",align:null},(0,m.mdx)("inlineCode",{parentName:"td"},"reproduction_rate()[]")),(0,m.mdx)("td",{parentName:"tr",align:null},"0.218"),(0,m.mdx)("td",{parentName:"tr",align:null},"0.004"),(0,m.mdx)("td",{parentName:"tr",align:null},"0.216"),(0,m.mdx)("td",{parentName:"tr",align:null},"0.218"),(0,m.mdx)("td",{parentName:"tr",align:null},"0.219"),(0,m.mdx)("td",{parentName:"tr",align:null},"1.003"),(0,m.mdx)("td",{parentName:"tr",align:null},"631.315")))),(0,m.mdx)("p",null,"The statistics presented are:"),(0,m.mdx)("ol",null,(0,m.mdx)("li",{parentName:"ol"},(0,m.mdx)("strong",{parentName:"li"},"Mean and standard deviation.")),(0,m.mdx)("li",{parentName:"ol"},(0,m.mdx)("strong",{parentName:"li"},"95% confidence interval.")),(0,m.mdx)("li",{parentName:"ol"},(0,m.mdx)("strong",{parentName:"li"},"Convergence diagnostic ",(0,m.mdx)("a",{parentName:"strong",href:"https://projecteuclid.org/euclid.ss/1177011136"},(0,m.mdx)("span",{parentName:"a",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("mover",{parentName:"mrow",accent:"true"},(0,m.mdx)("mi",{parentName:"mover"},"R"),(0,m.mdx)("mo",{parentName:"mover"},"^"))),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\hat{R}")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.9467699999999999em",verticalAlign:"0em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord accent"},(0,m.mdx)("span",{parentName:"span",className:"vlist-t"},(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.9467699999999999em"}},(0,m.mdx)("span",{parentName:"span",style:{top:"-3em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},(0,m.mdx)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.00773em"}},"R"))),(0,m.mdx)("span",{parentName:"span",style:{top:"-3.25233em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,m.mdx)("span",{parentName:"span",className:"accent-body",style:{left:"-0.16666em"}},(0,m.mdx)("span",{parentName:"span",className:"mord"},"^")))))))))))),".")),(0,m.mdx)("li",{parentName:"ol"},(0,m.mdx)("strong",{parentName:"li"},"Effective sample size ",(0,m.mdx)("a",{parentName:"strong",href:"https://www.mcmchandbook.net/HandbookChapter1.pdf"},(0,m.mdx)("span",{parentName:"a",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("msub",{parentName:"mrow"},(0,m.mdx)("mi",{parentName:"msub"},"N"),(0,m.mdx)("mtext",{parentName:"msub"},"eff"))),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"N_\\text{eff}")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.83333em",verticalAlign:"-0.15em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},(0,m.mdx)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10903em"}},"N"),(0,m.mdx)("span",{parentName:"span",className:"msupsub"},(0,m.mdx)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.33610799999999996em"}},(0,m.mdx)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.10903em",marginRight:"0.05em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,m.mdx)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,m.mdx)("span",{parentName:"span",className:"mord text mtight"},(0,m.mdx)("span",{parentName:"span",className:"mord mtight"},"eff"))))),(0,m.mdx)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,m.mdx)("span",{parentName:"span"}))))))))))),"."))),(0,m.mdx)("p",null,"These statistics provide different insights into the quality of the results of inference. For instance, we can use them in combination with synthetically generated data for which we know ground truth values for parameters and then check to make sure that these values fall within the 95% confidence intervals. Of course, in doing so it is important to keep in mind that the prior distributions in our model (and not just the data) will always have an influence on the posterior distribution. Similarly, we can use the size of the 95% confidence interval to gain insights into the model: If it is large, this could indicate that either we have too few observations or that the prior is too weak."),(0,m.mdx)("p",null,(0,m.mdx)("span",{parentName:"p",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("mover",{parentName:"mrow",accent:"true"},(0,m.mdx)("mi",{parentName:"mover"},"R"),(0,m.mdx)("mo",{parentName:"mover"},"^")),(0,m.mdx)("mo",{parentName:"mrow"},"\u2208"),(0,m.mdx)("mo",{parentName:"mrow",stretchy:"false"},"["),(0,m.mdx)("mn",{parentName:"mrow"},"1"),(0,m.mdx)("mo",{parentName:"mrow",separator:"true"},","),(0,m.mdx)("mi",{parentName:"mrow",mathvariant:"normal"},"\u221e"),(0,m.mdx)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\hat{R} \\in [1, \\infty)")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.9858699999999999em",verticalAlign:"-0.0391em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord accent"},(0,m.mdx)("span",{parentName:"span",className:"vlist-t"},(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.9467699999999999em"}},(0,m.mdx)("span",{parentName:"span",style:{top:"-3em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},(0,m.mdx)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.00773em"}},"R"))),(0,m.mdx)("span",{parentName:"span",style:{top:"-3.25233em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,m.mdx)("span",{parentName:"span",className:"accent-body",style:{left:"-0.16666em"}},(0,m.mdx)("span",{parentName:"span",className:"mord"},"^"))))))),(0,m.mdx)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}}),(0,m.mdx)("span",{parentName:"span",className:"mrel"},"\u2208"),(0,m.mdx)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}})),(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,m.mdx)("span",{parentName:"span",className:"mopen"},"["),(0,m.mdx)("span",{parentName:"span",className:"mord"},"1"),(0,m.mdx)("span",{parentName:"span",className:"mpunct"},","),(0,m.mdx)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.16666666666666666em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},"\u221e"),(0,m.mdx)("span",{parentName:"span",className:"mclose"},")")))))," summarizes how effective inference was at converging on the correct posterior distribution for a particular random variable. It uses information from all chains run in order to assess whether inference had a good understanding of the distribution or not. Values very close to ",(0,m.mdx)("span",{parentName:"p",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("mn",{parentName:"mrow"},"1.0")),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"1.0")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.64444em",verticalAlign:"0em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},"1"),(0,m.mdx)("span",{parentName:"span",className:"mord"},"."),(0,m.mdx)("span",{parentName:"span",className:"mord"},"0")))))," indicate that all chains discovered similar distributions for a particular random variable. We do not recommend using inference results where ",(0,m.mdx)("span",{parentName:"p",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("mover",{parentName:"mrow",accent:"true"},(0,m.mdx)("mi",{parentName:"mover"},"R"),(0,m.mdx)("mo",{parentName:"mover"},"^")),(0,m.mdx)("mo",{parentName:"mrow"},">"),(0,m.mdx)("mn",{parentName:"mrow"},"1.1")),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\hat{R} > 1.1")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.9858699999999999em",verticalAlign:"-0.0391em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord accent"},(0,m.mdx)("span",{parentName:"span",className:"vlist-t"},(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.9467699999999999em"}},(0,m.mdx)("span",{parentName:"span",style:{top:"-3em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},(0,m.mdx)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.00773em"}},"R"))),(0,m.mdx)("span",{parentName:"span",style:{top:"-3.25233em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,m.mdx)("span",{parentName:"span",className:"accent-body",style:{left:"-0.16666em"}},(0,m.mdx)("span",{parentName:"span",className:"mord"},"^"))))))),(0,m.mdx)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}}),(0,m.mdx)("span",{parentName:"span",className:"mrel"},">"),(0,m.mdx)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}})),(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.64444em",verticalAlign:"0em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},"1"),(0,m.mdx)("span",{parentName:"span",className:"mord"},"."),(0,m.mdx)("span",{parentName:"span",className:"mord"},"1"))))),", as inference may not have converged. In that case, you may want to run inference for more samples. However, there are situations in which increasing the number of samples will not improve convergence. In this case, it is possible that the prior is too far from the posterior that inference is unable to reliably explore the posterior distribution."),(0,m.mdx)("p",null,(0,m.mdx)("span",{parentName:"p",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("msub",{parentName:"mrow"},(0,m.mdx)("mi",{parentName:"msub"},"N"),(0,m.mdx)("mtext",{parentName:"msub"},"eff")),(0,m.mdx)("mo",{parentName:"mrow"},"\u2208"),(0,m.mdx)("mo",{parentName:"mrow",stretchy:"false"},"["),(0,m.mdx)("mn",{parentName:"mrow"},"1"),(0,m.mdx)("mo",{parentName:"mrow",separator:"true"},",")),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"N_\\text{eff} \\in [1,")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.83333em",verticalAlign:"-0.15em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},(0,m.mdx)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10903em"}},"N"),(0,m.mdx)("span",{parentName:"span",className:"msupsub"},(0,m.mdx)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.33610799999999996em"}},(0,m.mdx)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.10903em",marginRight:"0.05em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,m.mdx)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,m.mdx)("span",{parentName:"span",className:"mord text mtight"},(0,m.mdx)("span",{parentName:"span",className:"mord mtight"},"eff"))))),(0,m.mdx)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,m.mdx)("span",{parentName:"span"})))))),(0,m.mdx)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}}),(0,m.mdx)("span",{parentName:"span",className:"mrel"},"\u2208"),(0,m.mdx)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2777777777777778em"}})),(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,m.mdx)("span",{parentName:"span",className:"mopen"},"["),(0,m.mdx)("span",{parentName:"span",className:"mord"},"1"),(0,m.mdx)("span",{parentName:"span",className:"mpunct"},",")))))," ",(0,m.mdx)("inlineCode",{parentName:"p"},"num_samples"),(0,m.mdx)("span",{parentName:"p",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("mo",{parentName:"mrow",stretchy:"false"},"]")),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"]")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,m.mdx)("span",{parentName:"span",className:"mclose"},"]")))))," summarizes how independent posterior samples are from one another. Although inference was run for ",(0,m.mdx)("inlineCode",{parentName:"p"},"num_samples")," iterations, it's possible that those samples were very similar to each other (due to the way inference is implemented), and may not each be representative of the full posterior space. Larger numbers are better here, and if your particular use case calls for a certain number of samples to be considered, you should ensure that ",(0,m.mdx)("span",{parentName:"p",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("msub",{parentName:"mrow"},(0,m.mdx)("mi",{parentName:"msub"},"N"),(0,m.mdx)("mtext",{parentName:"msub"},"eff"))),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"N_\\text{eff}")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.83333em",verticalAlign:"-0.15em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},(0,m.mdx)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10903em"}},"N"),(0,m.mdx)("span",{parentName:"span",className:"msupsub"},(0,m.mdx)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.33610799999999996em"}},(0,m.mdx)("span",{parentName:"span",style:{top:"-2.5500000000000003em",marginLeft:"-0.10903em",marginRight:"0.05em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,m.mdx)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,m.mdx)("span",{parentName:"span",className:"mord text mtight"},(0,m.mdx)("span",{parentName:"span",className:"mord mtight"},"eff"))))),(0,m.mdx)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,m.mdx)("span",{parentName:"span"}))))))))))," is at least that large."),(0,m.mdx)("p",null,"In the case of our example model, we have a healthy ",(0,m.mdx)("span",{parentName:"p",className:"math math-inline"},(0,m.mdx)("span",{parentName:"span",className:"katex"},(0,m.mdx)("span",{parentName:"span",className:"katex-mathml"},(0,m.mdx)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,m.mdx)("semantics",{parentName:"math"},(0,m.mdx)("mrow",{parentName:"semantics"},(0,m.mdx)("mover",{parentName:"mrow",accent:"true"},(0,m.mdx)("mi",{parentName:"mover"},"R"),(0,m.mdx)("mo",{parentName:"mover"},"^"))),(0,m.mdx)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\hat{R}")))),(0,m.mdx)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,m.mdx)("span",{parentName:"span",className:"base"},(0,m.mdx)("span",{parentName:"span",className:"strut",style:{height:"0.9467699999999999em",verticalAlign:"0em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord accent"},(0,m.mdx)("span",{parentName:"span",className:"vlist-t"},(0,m.mdx)("span",{parentName:"span",className:"vlist-r"},(0,m.mdx)("span",{parentName:"span",className:"vlist",style:{height:"0.9467699999999999em"}},(0,m.mdx)("span",{parentName:"span",style:{top:"-3em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,m.mdx)("span",{parentName:"span",className:"mord"},(0,m.mdx)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.00773em"}},"R"))),(0,m.mdx)("span",{parentName:"span",style:{top:"-3.25233em"}},(0,m.mdx)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,m.mdx)("span",{parentName:"span",className:"accent-body",style:{left:"-0.16666em"}},(0,m.mdx)("span",{parentName:"span",className:"mord"},"^")))))))))))," value close to 1.0, and a healthy number of effective samples of 631."),(0,m.mdx)("h3",{id:"diagnostic-plots"},"Diagnostic plots"),(0,m.mdx)("p",null,"Bean Machine can also plot diagnostic information to assess health of the inference run. Let's take a look:"),(0,m.mdx)("pre",null,(0,m.mdx)("code",{parentName:"pre",className:"language-py"},"bm.Diagnostics(samples).plot(display=True)\n")),(0,m.mdx)("p",null,(0,m.mdx)("img",{src:n(91426).Z}),"\n",(0,m.mdx)("img",{src:n(94107).Z})),(0,m.mdx)("p",null,"The diagnostics output shows two diagnostic plots for individual random variables: trace plots and autocorrelation plots."),(0,m.mdx)("ul",null,(0,m.mdx)("li",{parentName:"ul"},"Trace plots are simply a time series of values assigned to random variables over each iteration of inference. The concrete values assigned are usually problem-specific. However, it's important that these values are \"mixing\" well over time. This means that they don't tend to get stuck in one region for large periods of time, and that each of the chains ends up exploring the same space as the other chains throughout the course of inference."),(0,m.mdx)("li",{parentName:"ul"},"Autocorrelation plots measure how predictive the last several samples are of the current sample. Autocorrelation may vary between -1.0 (deterministically anticorrelated) and 1.0 (deterministically correlated). (We compute autocorrelation approximately, so it may sometimes exceed these bounds.) In an ideal world, the current sample is chosen independently of the previous samples: an autocorrelation of zero. This is not possible in practice, due to stochastic noise and the mechanics of how inference works. The autocorrelation plots here plot how correlated samples from the end of the chain are compared with samples taken from elsewhere within the chain, as indicated by the iteration index on the x axis.")),(0,m.mdx)("p",null,"For our example model, we see from the trace plots that each of the chains are healthy: they don't get stuck, and do not explore a chain-specific subset of the space. From the autocorrelation plots, we see the absolute magnitude of autocorrelation to be very small, often around 0.1, indicating a healthy exploration of the space."),(0,m.mdx)("hr",null),(0,m.mdx)("p",null,"Congratulations, you've made it through the ",(0,m.mdx)("strong",{parentName:"p"},"Overview"),"! If you're looking to get an even deeper understanding of Bean Machine, check out the ",(0,m.mdx)("strong",{parentName:"p"},"Framework topics")," next. Or, if you're looking to get to coding, check out our ",(0,m.mdx)("strong",{parentName:"p"},"Tutorials"),". In either case, happy modeling!"))}c.isMDXComponent=!0},94107:function(e,a,n){a.Z=n.p+"assets/images/autocorrelation_reproduction_rate-24233acafee522e7ccaea4e038b729dd.png"},38016:function(e,a,n){a.Z=n.p+"assets/images/posterior_reproduction_rate-857bea008e4832561e8527776e24dd7e.png"},91426:function(e,a,n){a.Z=n.p+"assets/images/trace_reproduction_rate-726507a717df1728df162c9b2653a816.png"}}]);